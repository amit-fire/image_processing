Observations about the program

multiple presets (--preset base --preset olympian) can be added in the same run.
what does that mean?

run1: index.js --preset base 1.png
run2: index.js --preset base --preset base 2.png

the runs generate different outputs.
run2 is confusing. either fail or generate the same output as run1

sometimes visually there's no difference, but the output differ in size.

the program doesn't create out dir.

===
Task: compare each build to build 0 and find differences.

Note: The comparion to build 0 is a bit surprising.
If each build is a different release, shouldn't the output of
the current release be compared to the previous release?
That way when there's a diff, either a bug needs to be opened or
the current output should be saved as the expected output.

Output differ, what's next?
Review the outputs of builds 0 and X.
===
Programatically comparing images (Python)

Why Python? quick and easy.
For better performance: C++, Go, Rust, Zig should be considered.

Hamming distance is a way to compare data.
Hamming distance is the number of bit positions in which two bits are different.

Calculation:

imageio: read / write images
image from pillow (imaging library): image processing
imagehash: library for generating and comparing perceptual hashes of images

perceptual hash: comparison of images by looking at the number of different bits between the images.
the difference is known as Hamming distance.

1 - (hash1 - hash2) / max(len(hash1.hash) * 8, 1)

step 1: Hamming distance
hash1 - hash2

step 2: Normalizing Hamming distance
(hash1 - hash2) / max(len(hash1.hash) * 8, 1)
Divide Hamming distance by the maximum possbile bits
that could differ.
The length of hash1 provides the number of bits in the hash.
the hash is represented as a hexadecimal string, each digit is 4 bits.
Times 8 since there are 2 hashes. Gives the total bits.
max function avoids zero division.

step 3: similarity
The closer the result is to 1, the more similar the images.

===
Notes about the script
The script executes the program, compares each build to build 0 and if there are difference,
a diff image is created which highlights the differences.

The script receives a configuration file which instructs it how to run.
The script has 2 modes:

1. determined: this means the configuration file points to scenarios files.
The mode allows the user to control what scenarios are executed.

2. combinations generation: the scripts randomly selects parameters.

After each test, the results are reported.
Results are collected and displayed at the end of the last test.

===
Improvements:
1. Validations
2. Error handling
3. Logging
3.1 Add more logs.
3.2 Write logs to file , not just console.
4. Parameters are hard-coded. Better to collect them from the program.
Prior to running the tests, run the program without parameters and parse the output.
I would still hard code them and after collecting them, compare them to the hard coded version.
That case if parameters are added/removed the test fails and either a bug can be opened or the
hard coded version can be updated.
5. unit tests
6. refactor
===
Running the script:

running the script (compare_img.py) without parameters yields a usage message.

to run a specific set of scenario, use determined_tests.json
for an example scenario file see scenarios dir

to run a random scenario, use auto_generated_tests.json
===
example run:

to see an example of the script's output and a report see report dir)
report.txt was created manually. the rest of the files were generated by the script.
===

How many combinations exist?

basement: 3
elevation: 5
flexRoom: 5
garageDepth: 3
garageDoorSize: 6
garageSide: 2
garageSize: 3
mirror: 3
pool: 4
quareFootage: 5
stories: 3

3*5*5*3*6*2*3*3*4*5*3=1,458,000 combinations.
unrealsitic assumption: each combination takes a second to run.
result: 17 days

if the build number is included, the combinations would be 11,664,000 (1,458,000 * 8)
result: 135 days

Cost and time achieving 100% coverage:
Suppose the combinations are pre-computed.
Also, parallelize the computation (not easy to achieve, but doable).
Might be an overkill.
No need to compute the combinations with the build, that can be iterated upon during the test run.

Running the should be parallelized.
Store sets of parameter combinations in different files and have a manager node allocate the sets to different worker nodes.

Random selection of parameters:

Is there a large data set of parameters used that can be analyzed?

===
Error use cases:
1. Unsupported parameter [index.js dummy]
outcome: output is generated (file name doesn't have png extension)
expected: error / help message
2. Unsupported parameter value
outcome: help message (as expected)
3. Flag (preset) is provided multiple times [index.js --preset base --preset base out.png]
outcome: out.png is created
expected: error / help message
4. Program doesn't create out dir.

index.js --preset base out/out.png (out dir doesn't exist)

outcome:
node:fs:600
  handleErrorFromBinding(ctx);
  ^

Error: ENOENT: no such file or directory, open 'out/out.png'
    at Object.openSync (node:fs:600:3)
    at Object.writeFileSync (node:fs:2221:35)
    at generate (index.js:1:36565)
    at async main (index.js:1:42405) {
  errno: -4058,
  syscall: 'open',
  code: 'ENOENT',
  path: 'out1/z.png'
}

expected: crate path (or fail with error / help message)

note: fails even if full path is provided /usr/out/out.png

Misc:
1. documentation (help / error messages)
2. language support
3. check error level when the application ends
4. performance tests
5. security